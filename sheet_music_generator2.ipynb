{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "sheet_music_generator2.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t57EYlSQqxOi",
    "outputId": "be17df32-236f-47c4-ec85-fc0c6d1d8903"
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from music21 import stream, instrument, note, note\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IS_ON_GOOGLE_COLAB = True\n",
    "except:\n",
    "    IS_ON_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_ON_GOOGLE_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nexon\\AppData\\Local\\Temp/ipykernel_9856/2491690963.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "S23_wJ0FEFVS"
   },
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from music21 import converter, pitch, interval, instrument, note, note\n",
    "import tensorflow as tf\n",
    "# Define save directory\n",
    "from music21.key import Key\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "midi_dir = './midi_songs/'\n",
    "\n",
    "\n",
    "def get_current_datetime():\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    dt_name = now.strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "    return dt_name\n",
    "\n",
    "\n",
    "if IS_ON_GOOGLE_COLAB:\n",
    "    FOLDER_ROOT = os.path.join(\"content\", \"drive\", \"MyDrive\", \"magisterka\", \"SheetMusicGenerator2\")\n",
    "else:\n",
    "    FOLDER_ROOT = os.path.join(\".\")\n",
    "\n",
    "TEST_RUN = False\n",
    "NORMALIZE_NOTES = True\n",
    "NORMALIZATION_BOUNDARIES = [3, 4]\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "AUTOENCODER = \"AUTOENCODER\"\n",
    "MODEL_NAME = AUTOENCODER\n",
    "\n",
    "MODEL_FOLDER_ROOT = os.path.join(FOLDER_ROOT, MODEL_NAME)\n",
    "CURR_DT = get_current_datetime()\n",
    "MODEL_DIR_PATH = os.path.join(MODEL_FOLDER_ROOT, \"generated_models\")\n",
    "OCCURENCES = os.path.join(MODEL_FOLDER_ROOT, \"data\", \"occurences\")\n",
    "DATA_DIR = os.path.join(MODEL_FOLDER_ROOT, \"data\")\n",
    "\n",
    "DATA_NOTES_DIR = os.path.join(DATA_DIR, \"notes\")\n",
    "DATA_DURATIONS_DIR = os.path.join(DATA_DIR, \"durations\")\n",
    "DATA_DICTS_DIR = os.path.join(DATA_DIR, \"dicts\")\n",
    "\n",
    "DATA_INT_TO_NOTE_PATH = os.path.join(DATA_DICTS_DIR, \"int_to_note_\" + str(CURR_DT))\n",
    "DATA_INT_TO_DURATION_PATH = os.path.join(DATA_DICTS_DIR, \"int_to_duration_\" + str(CURR_DT))\n",
    "DATA_NOTES_PATH = os.path.join(DATA_NOTES_DIR, \"notes_\" + str(CURR_DT))\n",
    "DATA_DURATIONS_PATH = os.path.join(DATA_DURATIONS_DIR, \"durations_\" + str(CURR_DT))\n",
    "\n",
    "MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs\")\n",
    "# MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs_smaller\")\n",
    "MIDI_GENERATED_DIR = os.path.join(MODEL_FOLDER_ROOT, \"midi_generated\")\n",
    "MIDI_SONGS_REGEX = os.path.join(MIDI_SONGS_DIR, \"*.mid\")\n",
    "CHECKPOINTS_DIR = os.path.join(MODEL_FOLDER_ROOT, \"checkpoints\")\n",
    "CHECKPOINT = os.path.join(CHECKPOINTS_DIR, str(CURR_DT))\n",
    "LOGS_DIR = os.path.join(MODEL_FOLDER_ROOT, \"logs\")\n",
    "LOG = os.path.join(LOGS_DIR, str(CURR_DT))\n",
    "\n",
    "COMPUTED_INT_TO_NOTE_PATH = \"C:\\\\Users\\\\Nexon\\\\PycharmProjects\\\\SheetMusicGenerator2\\\\AUTOENCODER\\\\data\\\\dicts\\\\int_to_note_08_16_2021__20_10_21\"\n",
    "COMPUTED_INT_TO_DURATION_PATH = \"C:\\\\Users\\\\Nexon\\\\PycharmProjects\\\\SheetMusicGenerator2\\\\AUTOENCODER\\\\data\\\\dicts\\\\int_to_duration_08_16_2021__20_10_21\"\n",
    "COMPUTED_NOTES_PATH = \"C:\\\\Users\\\\Nexon\\\\PycharmProjects\\\\SheetMusicGenerator2\\\\AUTOENCODER\\\\data\\\\notes\\\\notes_08_16_2021__20_10_21\"\n",
    "COMPUTED_DURATIONS_PATH = \"C:\\\\Users\\\\Nexon\\\\PycharmProjects\\\\SheetMusicGenerator2\\\\AUTOENCODER\\\\data\\\\durations\\\\durations_08_16_2021__20_10_21\"\n",
    "\n",
    "all_paths = [MODEL_DIR_PATH, MODEL_NAME, OCCURENCES, DATA_NOTES_DIR, DATA_DURATIONS_DIR, DATA_DICTS_DIR,\n",
    "             MIDI_GENERATED_DIR, CHECKPOINTS_DIR, CHECKPOINT, LOGS_DIR, LOG]\n",
    "for path in all_paths:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     create_train_data()\n",
    "# # Convert to one-hot encoding and swap note and sequence dimensions"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b1JArktFEFVY"
   },
   "source": [
    "class MusicAutoencoder():\n",
    "    def __init__(self, latent_dim, sequence_length, train_notes_path=None, train_durations_path=None,\n",
    "                 int_to_note_path=None, int_to_duration_path=None):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.tensor_dataset = None\n",
    "        self.input_dim = None\n",
    "        self.n_notes = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "\n",
    "        if train_notes_path is None or train_durations_path is None or int_to_note_path is None or int_to_duration_path is None:\n",
    "            self.parse_songs()\n",
    "\n",
    "        else:\n",
    "            with open(train_notes_path, 'rb') as train_notes_file:\n",
    "                self.train_notes = pickle.load(train_notes_file)\n",
    "\n",
    "            with open(train_notes_path, 'rb') as train_durations_file:\n",
    "                self.train_durations = pickle.load(train_durations_file)\n",
    "\n",
    "            with open(train_notes_path, 'rb') as int_to_note_file:\n",
    "                self.int_to_note = pickle.load(int_to_note_file)\n",
    "\n",
    "            with open(train_notes_path, 'rb') as int_to_duration_file:\n",
    "                self.int_to_duration = pickle.load(int_to_duration_file)\n",
    "\n",
    "        self.prepare_data()\n",
    "        self.model = self.autoencoder()\n",
    "\n",
    "    # def create_autoencoder(self):\n",
    "    #         self.model = self.autoencoder(self.input_dim, self.latent_dim)\n",
    "\n",
    "    def autoencoder(self):\n",
    "        # Define encoder input shape\n",
    "        encoder_input = tf.keras.layers.Input(shape=self.input_dim)\n",
    "\n",
    "        # Define decoder input shape\n",
    "        latent = tf.keras.layers.Input(shape=self.latent_dim)\n",
    "\n",
    "        # Define dense encoding layer connecting input to latent vector\n",
    "        encoded = tf.keras.layers.Dense(self.latent_dim, activation='tanh')(encoder_input)\n",
    "\n",
    "        # Define dense decoding layer connecting latent vector to output\n",
    "        decoded = tf.keras.layers.Dense(self.input_dim, activation='sigmoid')(latent)\n",
    "\n",
    "        # Define the encoder and decoder models\n",
    "        self.encoder = tf.keras.Model(encoder_input, encoded)\n",
    "        self.decoder = tf.keras.Model(latent, decoded)\n",
    "\n",
    "        # Define autoencoder model\n",
    "        autoencoder = tf.keras.Model(encoder_input, self.decoder(encoded))\n",
    "        return autoencoder\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"Replaces Keras' native ImageDataGenerator.\"\"\"\n",
    "        # i = 0\n",
    "        # file_list = os.listdir(directory)\n",
    "\n",
    "        for batch in self.tensor_dataset.batch(BATCH_SIZE):\n",
    "            print(batch)\n",
    "            yield batch\n",
    "        # while True:\n",
    "        #     batch = []\n",
    "        #     for b in range(batch_size):\n",
    "        #         self.tensor_dataset.batch(BATCH_SIZE)\n",
    "        #         if i == len(file_list):\n",
    "        #             i = 0\n",
    "        #         sample = file_list[i]\n",
    "        #         i += 1\n",
    "        #         # image = cv2.resize(cv2.imread(sample[0]), INPUT_SHAPE)\n",
    "        #         # image_batch.append((image.astype(float) - 128) / 128)\n",
    "        #\n",
    "        #     yield np.array(image_batch)\n",
    "\n",
    "\n",
    "    def train(self, checkpoint_path=None):\n",
    "        # Define number of samples, notes and notes, and input dimension\n",
    "        # filepath = CHECKPOINTS + \"weights-improvement-{epoch:02d}-{loss:.4f}-{categorical_accuracy:.4f}-bigger.hdf5\"\n",
    "        # filepath = \"weights-improvement-epoch:{epoch:02d}-loss:{loss:.4f}-cat_acc:{categorical_accuracy:.4f}.hdf5\"\n",
    "\n",
    "        print(str(\"Current datatime: \" + CURR_DT))\n",
    "\n",
    "        if checkpoint_path:\n",
    "            self.model.load_weights(checkpoint_path)\n",
    "\n",
    "        filepath = os.path.join(CHECKPOINT, \"epoch={epoch:02d}-loss={loss:.4f}-acc={binary_accuracy:.4f}.hdf5\")\n",
    "\n",
    "        # filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath,\n",
    "            monitor='binary_accuracy',\n",
    "            verbose=0,\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        log = tf.keras.callbacks.TensorBoard(log_dir=LOG),\n",
    "\n",
    "        callbacks_list = [checkpoint, log]\n",
    "        # history = self.model.fit(network_input, network_output, epochs=EPOCHS, batch_size=128, callbacks=callbacks_list)\n",
    "        # model.save(MODEL_DIR_PATH + MODEL_NAME + \"_\" + CURR_DT + \".hdf5\")\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.01),\n",
    "                           metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "        # self.model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.01), metrics=[\"accuracy\"])\n",
    "        # Train autoencoder\n",
    "        self.model.summary()\n",
    "        print(MODEL_DIR_PATH + MODEL_NAME + \"_\" + CURR_DT + \".hdf5\")\n",
    "        # history = self.model.fit(self.trainNotesFlat, self.trainNotesFlat, epochs=1)\n",
    "        # history = self.model.fit(self.trainNotesFlat, self.trainNotesFlat, epochs=500, callbacks=callbacks_list, batch_size=8)\n",
    "        # tensor_dataset = tf.data.Dataset.from_tensors((self.trainNotesFlat, self.trainNotesFlat))\n",
    "\n",
    "        history = self.model.fit(self.generate_data(), epochs=EPOCHS, callbacks=callbacks_list, batch_size=BATCH_SIZE)\n",
    "        print(history.history)\n",
    "        print(MODEL_DIR_PATH + MODEL_NAME + \"_\" + CURR_DT + \".hdf5\")\n",
    "        self.model.save(os.path.join(MODEL_DIR_PATH, MODEL_NAME + \"_\" + CURR_DT + \".hdf5\"))\n",
    "\n",
    "    def generate_notes(self):\n",
    "        generated_notes = self.decoder(np.random.normal(size=(1, self.latent_dim)))\\\n",
    "            .numpy().reshape(self.n_notes, self.sequence_length)\\\n",
    "            .argmax(0)\n",
    "\n",
    "        generated_stream = stream.Stream()\n",
    "        generated_stream.append(instrument.Piano())\n",
    "        note_sequence = [self.int_to_note[c] for c in generated_notes]\n",
    "        # Append notes and notes to stream object\n",
    "        for j in range(len(note_sequence)):\n",
    "            try:\n",
    "                generated_stream.append(note.Note(note_sequence[j].replace('.', ' ')))\n",
    "            except:\n",
    "                generated_stream.append(note.Note(note_sequence[j].replace('.', ' ')))\n",
    "\n",
    "        generated_stream.write('midi', fp=MIDI_GENERATED_DIR + 'autoencoder.mid')\n",
    "        # return generatedNotes\n",
    "\n",
    "    def parse_songs(self):\n",
    "        # Create empty list for scores\n",
    "        original_scores = []\n",
    "\n",
    "        # Load and make list of stream objects\n",
    "        for song in glob.glob(MIDI_SONGS_REGEX):\n",
    "            print(\"Parsing song: \" + str(song))\n",
    "            score = converter.parse(song)\n",
    "            original_scores.append(score)\n",
    "\n",
    "        # Define empty lists of lists\n",
    "        original_notes = [[] for _ in original_scores]\n",
    "        original_durations = [[] for _ in original_scores]\n",
    "        original_keys = []\n",
    "\n",
    "        def transpose_amount(score):\n",
    "            return -int(score.noteify().analyze('key').tonic.ps % 12)\n",
    "\n",
    "        def monophonic(stream):\n",
    "            try:\n",
    "                length = len(instrument.partitionByInstrument(stream).parts)\n",
    "            except:\n",
    "                length = 0\n",
    "            return length == 1\n",
    "\n",
    "        # Extract notes, notes, durations, and keys\n",
    "\n",
    "        original_scores = [song.chordify() for song in original_scores]\n",
    "\n",
    "        for i, song in enumerate(original_scores):\n",
    "\n",
    "            # song.transpose\n",
    "            transp_int = transpose_amount(song)\n",
    "            original_keys.append(str(song.analyze('key').transpose(transp_int)))\n",
    "            for element in song:\n",
    "                if isinstance(element, note.Note):\n",
    "                    original_notes[i].append(element.pitch.transpose(transp_int))\n",
    "                    original_durations[i].append(element.duration.quarterLength)\n",
    "                elif isinstance(element, note.Note):\n",
    "                    original_notes[i].append('.'.join(str(n.transpose(transp_int)) for n in element.pitches))\n",
    "                    original_durations[i].append(element.duration.quarterLength)\n",
    "            print(str(original_keys[i]))\n",
    "\n",
    "        c_notes = [c for (c, k) in zip(original_notes, original_keys) if (k == 'C major')]\n",
    "        c_durations = [c for (c, k) in zip(original_durations, original_keys) if (k == 'C major')]\n",
    "        # Map unique notes to integers\n",
    "        unique_notes = np.unique([i for s in original_notes for i in s])\n",
    "        note_to_int = dict(zip(unique_notes, list(range(0, len(unique_notes)))))\n",
    "\n",
    "        # Map unique durations to integers\n",
    "        unique_durations = np.unique([i for s in original_durations for i in s])\n",
    "        duration_to_int = dict(zip(unique_durations, list(range(0, len(unique_durations)))))\n",
    "\n",
    "        # Print number of unique notes and notes\n",
    "        print(len(unique_notes))\n",
    "\n",
    "        # Print number of unique durations\n",
    "        print(len(unique_durations))\n",
    "\n",
    "        int_to_note = {i: c for c, i in note_to_int.items()}\n",
    "        int_to_duration = {i: c for c, i in duration_to_int.items()}\n",
    "\n",
    "        # Define sequence length\n",
    "\n",
    "        # Define empty arrays for train data\n",
    "        train_notes = []\n",
    "        train_durations = []\n",
    "\n",
    "        # Construct training sequences for notes and durations\n",
    "        for s in range(len(c_notes)):\n",
    "            note_list = [note_to_int[c] for c in c_notes[s]]\n",
    "            duration_list = [duration_to_int[d] for d in c_durations[s]]\n",
    "            for i in range(len(note_list) - self.sequence_length):\n",
    "                train_notes.append(note_list[i:i + self.sequence_length])\n",
    "                train_durations.append(duration_list[i:i + self.sequence_length])\n",
    "\n",
    "        with open(DATA_NOTES_PATH, 'wb') as filepath:\n",
    "            pickle.dump(train_notes, filepath)\n",
    "\n",
    "        with open(DATA_DURATIONS_PATH, 'wb') as filepath:\n",
    "            pickle.dump(train_durations, filepath)\n",
    "\n",
    "        with open(DATA_INT_TO_NOTE_PATH, 'wb') as filepath:\n",
    "            pickle.dump(int_to_note, filepath)\n",
    "\n",
    "        with open(DATA_INT_TO_DURATION_PATH, 'wb') as filepath:\n",
    "            pickle.dump(int_to_duration, filepath)\n",
    "\n",
    "        self.train_notes = train_notes\n",
    "        self.train_durations = train_durations\n",
    "        self.int_to_note = int_to_note\n",
    "        self.int_to_duration = int_to_duration\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # print(\"trainNotesFlat: \" + str(train_notes))\n",
    "        train_notes_categorical = tf.keras.utils.to_categorical(self.train_notes, dtype=\"float16\").transpose(0, 2, 1)\n",
    "        # Convert data to numpy array of type float\n",
    "        # trainNotes = np.array(trainNotes, np.float32)\n",
    "\n",
    "        n_samples = train_notes_categorical.shape[0]\n",
    "        n_notes = train_notes_categorical.shape[1]\n",
    "        self.input_dim = n_notes * self.sequence_length\n",
    "        # Flatten sequence of notes into single dimension\n",
    "        train_notes_flattened = train_notes_categorical.reshape(n_samples, self.input_dim)\n",
    "        self.tensor_dataset = tf.data.Dataset.from_tensors(tensors=(train_notes_flattened, train_notes_flattened))\n",
    "\n",
    "        # return tensor_dataset, input_dim, train_durations, sequence_length, int_to_note, int_to_duration, n_notes"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hiKIAF-yEFVa"
   },
   "source": [
    "class ModelFactory:\n",
    "    def factory(self, model_type, use_computed_values):\n",
    "        if model_type == AUTOENCODER:\n",
    "            if use_computed_values:\n",
    "                model = MusicAutoencoder(2, 32, COMPUTED_NOTES_PATH, COMPUTED_DURATIONS_PATH, COMPUTED_INT_TO_NOTE_PATH,\n",
    "                                         COMPUTED_INT_TO_DURATION_PATH)\n",
    "            else:\n",
    "                model = MusicAutoencoder(2, 32)\n",
    "            return model"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NCWrUciWEFVc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8d91654a-38a1-45f3-e606-fb38c7c9d355"
   },
   "source": [
    "modelFactory = ModelFactory()\n",
    "music_autoencoder = modelFactory.factory(MODEL_NAME, True)\n",
    "music_autoencoder.train()\n",
    "music_autoencoder.generate_notes()"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInternalError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9856/876769406.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmodelFactory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mModelFactory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmusic_autoencoder\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodelFactory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfactory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mMODEL_NAME\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mmusic_autoencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mmusic_autoencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerate_notes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9856/467177938.py\u001B[0m in \u001B[0;36mfactory\u001B[1;34m(self, model_type, use_computed_values)\u001B[0m\n\u001B[0;32m      4\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0muse_computed_values\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m                 model = MusicAutoencoder(2, 32, COMPUTED_NOTES_PATH, COMPUTED_DURATIONS_PATH, COMPUTED_INT_TO_NOTE_PATH,\n\u001B[1;32m----> 6\u001B[1;33m                                          COMPUTED_INT_TO_DURATION_PATH)\n\u001B[0m\u001B[0;32m      7\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m                 \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMusicAutoencoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9856/2694788289.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, latent_dim, sequence_length, train_notes_path, train_durations_path, int_to_note_path, int_to_duration_path)\u001B[0m\n\u001B[0;32m     26\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mint_to_duration\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mint_to_duration_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprepare_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautoencoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9856/2694788289.py\u001B[0m in \u001B[0;36mprepare_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    237\u001B[0m         \u001B[1;31m# Flatten sequence of notes into single dimension\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    238\u001B[0m         \u001B[0mtrain_notes_flattened\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_notes_categorical\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_samples\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 239\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_tensors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_notes_flattened\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_notes_flattened\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    240\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    241\u001B[0m         \u001B[1;31m# return tensor_dataset, input_dim, train_durations, sequence_length, int_to_note, int_to_duration, n_notes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36mfrom_tensors\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    604\u001B[0m       \u001B[0mDataset\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    605\u001B[0m     \"\"\"\n\u001B[1;32m--> 606\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mTensorDataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    607\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    608\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, element)\u001B[0m\n\u001B[0;32m   3823\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0melement\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3824\u001B[0m     \u001B[1;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3825\u001B[1;33m     \u001B[0melement\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstructure\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormalize_element\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melement\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3826\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_structure\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstructure\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype_spec_from_value\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0melement\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3827\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tensors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstructure\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_tensor_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_structure\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0melement\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001B[0m in \u001B[0;36mnormalize_element\u001B[1;34m(element, element_signature)\u001B[0m\n\u001B[0;32m    127\u001B[0m           \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspec\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"dtype\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    128\u001B[0m           normalized_components.append(\n\u001B[1;32m--> 129\u001B[1;33m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001B[0m\u001B[0;32m    130\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpack_sequence_as\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpack_as\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnormalized_components\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001B[0m in \u001B[0;36mwrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mTrace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrace_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mtrace_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m           \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mconvert_to_tensor\u001B[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001B[0m\n\u001B[0;32m   1564\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1565\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1566\u001B[1;33m       \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconversion_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mas_ref\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1567\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1568\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mNotImplemented\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001B[0m in \u001B[0;36m_default_conversion_function\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_default_conversion_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mas_ref\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m   \u001B[1;32mdel\u001B[0m \u001B[0mas_ref\u001B[0m  \u001B[1;31m# Unused.\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 52\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mconstant_op\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconstant\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     53\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36mconstant\u001B[1;34m(value, dtype, shape, name)\u001B[0m\n\u001B[0;32m    270\u001B[0m   \"\"\"\n\u001B[0;32m    271\u001B[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001B[1;32m--> 272\u001B[1;33m                         allow_broadcast=True)\n\u001B[0m\u001B[0;32m    273\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36m_constant_impl\u001B[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[0;32m    281\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"tf.constant\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    282\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 283\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    284\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    285\u001B[0m   \u001B[0mg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_default_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36m_constant_eager_impl\u001B[1;34m(ctx, value, dtype, shape, verify_shape)\u001B[0m\n\u001B[0;32m    306\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_constant_eager_impl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mctx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    307\u001B[0m   \u001B[1;34m\"\"\"Creates a constant on the current device.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 308\u001B[1;33m   \u001B[0mt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_to_eager_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    309\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mshape\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nexon\\pycharmprojects\\sheetmusicgenerator2\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001B[0m in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    104\u001B[0m       \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_dtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_datatype_enum\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    105\u001B[0m   \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 106\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEagerTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInternalError\u001B[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ]
  }
 ]
}